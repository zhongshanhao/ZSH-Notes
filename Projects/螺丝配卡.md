**大螺丝简介**

大螺丝是一个中间件，一边对接算法，一边对接工程（产品）。

大螺丝功能：

- 提供统一标准的operator（算子）接口，一般一个operator提供一个功能（如人脸特征抽取包括目标检查和特征抽取等），统一对接标准，强API兼容。

- 提供 operator 的服务化方案，主要用于后续可扩展框架。

- 提供统一的测试框架，后续测试方案统一。

最上层是业务层，就是产品。产品会call Lucifron的thrift接口，告诉服务需要用什么算法（哪些Operator）。Lucifron收到请求之后，就通知下层的Majordomo去获取Flamewaker的节点信息。拿到节点信息之后Majordomo将配置存入mongo。部分请求（比如1：N）直接对Operator发起请求。另外一些请求，会通过Lucifron再向Flamewaker发请求，调用算力进行运算。Flamewaker 本质就是 operator 的容器，编译时, 可以把需要的 operator 通过宏注册进 Flamewaker，运行时, 可以通过参数告诉这个 Flamewaker 要如何启动哪一些 operator, 并如何服务。

values.yaml

```
# 1.3 之前在此配置 mongodb 的链接信息, 从 1.3 开始优先在 persistent 字段下配置存储
# mongodbUri: mongodb://mongo-mongodb-replicaset-0.mongo-mongodb-replicaset.middlewares:27017/ragnaros
 
persistent:                                                  # 从 1.3 开始支持本配置项, 以下两种存储只能二选一
  mongodb:                                                   # mongodb 相关的配置
    uri: mongodb://mongo-mongodb-replicaset-0.mongo-mongodb-replicaset.middlewares:27017/ragnaros
  #mysql:                                                    # 以下是针对 mysql / rds / drds 作为存储后端的配置
  #  jdbc-url: jdbc:mysql://localhost:3306/ragnaros          # 包含大螺丝在数据库中使用的 schema 名称
  #  username: root
  #  password: "123456"
  #  driver-version: 5.1.34                                  # 从 1.3.4 开始支持，不配置则默认为8
  #  driver-class-name: com.mysql.jdbc.Driver                # 从 1.3.4 开始支持，不配置则默认为com.mysql.cj.jdbc.Driver
# majordomo related
majordomo:
  replica: 1                                                  # 从 1.2.2 开始支持 majordomo 高可用, 不填默认为 1
  image: ragnaros-core:v1.0.b                                 # majordomo 使用的 image
  serviceName: ragnaros-majordomo                             # majordomo expose 的服务名, 在 k8s 集群内可以通过该 serviceName 去访问 majordomo
  port: 9980                                                  # majordomo expose 的端口号, 如果不填, 默认为 9980
  nodeSelector:                                               # deprecated: 设置 majordomo 的 nodeSelector, 尽量在 podAttributes 里面设置
    beta.kubernetes.io/arch: amd64                              # 需要跑在 x86 机器上
  memoryLimitInMb: 4096                                       # majordomo jvm 堆内存限制, 如果不填，默认为 1024.
  enableMonitor: true                                         # 开启监控, 需要 paas 支持, 如果不填, 默认为 false
  containerAttributes:                                        # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段. 这里可以自定义的 mount volumes
    volumeMounts:
    - mountPath: /etc/xxx
      name: xxx
  podAttributes:                                              # pod 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段                                           
    volumes:
    - name: xxx
      hostPath:
        path: /etc/yyy
  commandSuffix:                                              # 用于 majordomo 启动的额外 config. 根据需求填写，如果没有需求, 可以省略该字段
  - xx=yy
  - cc=dd
 
# lucifron related
lucifron:
  image: ragnaros/core:1.0                                    # lucifron 使用的 image
  serviceName: ragnaros-lucifron                              # lucifron expose 的服务名, 在 k8s 集群内可以通过该 serviceName 去访问 lucifron
  port: 9990                                                  # lucifron expose 的端口号, 如果不填, 默认为 9990
  nodePort: 30990                                             # lucifron 暴露给集群外的端口号, 集群外部访问使用该端口号
  replica: 1                                                  # lucifron replica 数量, 根据需求填写
  nodeSelector:                                               # deprecated: 设置 lucifron 的 nodeSelector, 尽量在 podAttributes 里面设置
    beta.kubernetes.io/arch: amd64                            # 需要跑在 x86 机器上
  memoryLimitInMb: 1024                                       # lucifron jvm 堆内存限制, 如果不填，默认为 1024.
  enableMonitor: true                                         # 开启监控, 需要 paas 支持, 如果不填, 默认为 false
  containerAttributes:                                        # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段. 这里可以自定义的 mount volumes
    volumeMounts:
    - mountPath: /etc/xxx
      name: xxx
  podAttributes:                                              # pod 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段                                           
    volumes:
    - name: xxx
      hostPath:
        path: /etc/yyy
  commandSuffix:                                              # 用于 lucifron 启动的额外 config. 根据需求填写，如果没有需求, 可以省略该字段
  - --xxx=yyy
  - --wahaha=yixixi
 
 
inClusterLucifron:                                            # since 1.2.1
- image: ragnaros/core:1.0
  port: 9990                                                  # grpc port, 如果不填, 默认为 9990
  httpPort: 9991                                              # http port, 如果不填, 默认为 9991
  memoryLimitInMb: 1024                                       # in cluster lucifron jvm 堆内存限制, 如果不填，默认为 1024.
  containerAttributes:                                        # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段. 这里可以自定义的 mount volumes
    volumeMounts:
    - mountPath: /etc/xxx
      name: xxx
  podAttributes:                                              # pod 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    nodeSelector:
      beta.kubernetes.io/arch: amd64
  commandSuffix:                                              # 用于 lucifron 启动的额外 config. 根据需求填写，如果没有需求, 可以省略该字段
  - --xxx=yyy
  - --wahaha=yixixi
  nearbyConditions:                                           # 用于判断 nearby 资源的 conditions, 目前只支持 type = subNetwork 和 type = nodeLabel
  - type: subNetwork                                          # 会根据子网相同寻找 nearby
    mask: 24
  - type: nodeLabel                                           # 会根据 label 相同的寻找 nearby
    label: nearby
 
groups:
- name: sample-group-a                                        # group name, 由小写字符, '-', '.' 组成
  model: SIMPLE_ASYNC                                         # 3 种 model: SIMPLE_ASYNC, STATEFUL_ASYNC, REALTIME_ANALYSE
  updateStrategy: None                                        # 该 group 下的 flamewaker 更新策略, 默认为 RollingUpdate, None 表示永远不 update. 注意: 滚动升级在一个 flamewaker 存在多个 operator 的情况可能会存在不确定性的问题
  scaler:                                                     # 可缺省, 表示不启用 scaler
    desiredLoadPercent: 80                                    # 设置自动扩容的期望负载百分比, 根据需求填写, 可以省略 ( 默认 80 )
    downscaleStabilizationSenconds: 60                        # 设置缩容稳定时间, 根据需求填写，可以省略 ( 默认 60 )
    minCapacities:                                            # scaler 的下界, 缺省意味着 0
      concurrency: 100
    maxCapacities:                                            # scaler 的上界, 缺省意味着 int max
      concurrency: 500
- name: sample-group-b     
  model: STATEFUL_ASYNC
  configuration:                                              # group 的配置, 根据需求填写, 可以省略. 具体使用可以找 ragnaros 咨询
    archive:
      delayed:
        logNumberThreshold: 1000000
      idle:                                                   # 可以省略，当前提供的为省略的默认值。连续低于 ${updateThreshold} qps 的 update 时间超过 ${intervalInMs} 后, 就会触发 archive
        intervalInMs: 10000 
        updateThreshold: 2000
  scaler:                                                     # stateful async 下不可缺省, 用于自动配置 replica 和 sharding
    desiredLoadPercent: 80                                    # 设置自动扩容的期望负载百分比, 根据需求填写, 可以省略 ( 默认 80 )
    downscaleStabilizationSenconds: 60                        # 设置缩容稳定时间, 根据需求填写，可以省略 ( 默认 60 )
    minCapacities:                                            # scaler 的下界, 不可缺省
      concurrency: 100
      state-number: 100000                                    # stateful async 特有, 必须填写
    maxCapacities:                                            # scaler 的上界, 不可缺省
      concurrency: 300
      state-number: 100000                                    # 必须和 minCapacities 中的 state-number 相等
- name: sample-group-c
  model: REALTIME_ANALYSE
  configuration:
    operatorLostTimeoutInMs: -1                               # operator 多少 ms 后认为是丢失 ( 上面的任务会被迁移 ), -1 表示 operator 不会丢失 ( 任务也不会迁移 )
    schedulePeriodInMs: 1000
    defaultMaxStateNumber: 10
 
 
  
flamewakers:
- image: ragnaros/flamewaker:test                   # flamewaker 使用的 image
  name: xxx                                         # flamewaker 的 name
  apiVersion: v2                                    # 从 1.2 版本开始, flamewaker image 的版本, v1 表示 thrift 版本, v2 表示 grpc 版本, 基于 ragnaros 1.2 的版本打出的 image 都是 v2, 不填默认为 v2
  # instanceNumber: 1                               # 该类 flamewaker 的 instance 数量, 一般只有 STATEFUL_ASYNC 根据需求设置, 其他 model 一般设置为 1 (注意: 这里 instanceNumber 和 replica 的语义与 1.1 之前的版本相反)
                                                    # 从 1.2.1 开始, 此字段已经废弃, 若仍然填写, 系统会兼容的把 replica *= instanceNumber
  replica: 2                                        # 该类 flamewaker 的 replica number ( 即启动多少个 )
                                                    # 从 1.2.1 开始, 此字段可以缺省, 意思是直接交由 scaler 进行自适应
  commandSuffix:                                    # 从 1.2.1 开始, 该配置由 string 变成了 list, 可以在 flamewaker 启动命令后加参数(如 flags), 根据需求填写, 如果没有需求, 可以省略该字段
  - --enableLocalCache=false        
  operators:                                        # operators 为 list, 支持在一个 flamewaker 进程中起多个 operator, 具体使用请找 ragnaros 咨询
  - group: sample-group-a                           # group name, 由小写字符, '-', '.' 组成
    model: SIMPLE_ASYNC                             # 与 groups 中配置一致
    className: SampleSimpleAsyncOperator            # operator class name
    configFile: config/sample-simple-async-config.json     # config 目录下的配置文件, 必须以 config 开头
    capacities:                                   
      concurrency: 100                              # 作用于 SIMPLE_ASYNC / STATEFUL_ASYNC, 描述这个 flamewaker 可以同时处理多少个请求(并发数)
  podAttributes:                                    # pod 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    nodeSelector:
      kubernetes.io/arch: arm64
    volumes:
    - name: thinkforce0
      hostPath:
        path: /dev/thinkforce0
    - name: mv500
      hostPath:
        path: /dev/mv500
  containerAttributes:                              # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    resources:
      requests:
        memory: 64Mi
        cpu: 250m
      limits:
        memory: 128Mi
        cpu: 500m
        nvidia.com/gpu: 0
    volumeMounts:
    - mountPath: /dev/thinkforce0
      name: thinkforce0
    - mountPath: /dev/mv500
      name: mv500
    securityContext:
      privileged: true
  extraContainers:                                  # 额外的 container, 按 k8s 的语法填. 根据需求增加，如果没有需求，可以省略该字段           
  - name: wahaha
    image: yixixi:1.0.b
    command: ["/bin/sh", "-c", "sleep 60"]
    ports:
    - containerPort: 28888
- image: ragnaros/flamewaker:test
  replica: 2
  operators:
  - group: sample-group-b
    model: STATEFUL_ASYNC
    className: SampleStatefulAsyncOperator
    configFile: config/sample-stateful-async-config.json
    capacities:
      concurrency: 100
      state-number: 100                             # 作用于 STATEFUL_ASYNC, 描述这个 flamewaker 可以支持大的 state 数量
  containerAttributes:                              # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    resources:
      limits:
        nvidia.com/gpu: 0                 
- image: ragnaros/flamewaker:test
  instanceNumber: 1
  replica: 2
  operators:
  - group: sample-group-c
    model: REALTIME_ANALYSE
    className: SampleRealtimeAnalyseOperator
    configFile: config/sample-realtime-analyse-config.json
    capacities:
      task-number: 100                              # 作用于 REALTIME_ANALYSE, 描述这个 flamewaker 可以支持多少个 task
  containerAttributes:                              # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    resources:
      limits:
        nvidia.com/gpu: 1   
- name: sample-plans
  replica: 1
  plans:                                            # since 1.3.5
    - template: plans/fe.jsonnet                    # plans 目录下的配置文件名, 必须以 plans 开头, 内容示例参考 Ragnaros L1 Design 中的 fe.jsonnet
      options: |                                    # options 可以是一个包含合法 JSON 的字符串, 也可以是 JSON 对应 YAML 的字典语法
        {"opA": "wahaha",
        "opB": "yixixi"}
      patches:
        - op: replace
          path: /containerAttributes/resources/limits/nvidia.com~1gpu
          value: 1
      group: sample-group-a
      capacities:
        concurrency: 1000
        state-number: 1000000
  imagePrefix: library/
  commandSuffix:
    - --enableLocalCache=false
  podAttributes:                                   # pod 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    nodeSelector:
      kubernetes.io/arch: arm64
    volumes:
      - name: thinkforce0
        hostPath:
          path: /dev/thinkforce0
      - name: mv500
        hostPath:
          path: /dev/mv500
  containerAttributes:                            # container 相关的 attributes, 按 k8s 语法填. 根据需求填写，如果没有需求, 可以省略该字段
    resources:
      requests:
        memory: 64Mi
        cpu: 250m
    volumeMounts:
      - mountPath: /dev/thinkforce0
        name: thinkforce0
      - mountPath: /dev/mv500
        name: mv500
    securityContext:
      privileged: true
  extraContainers: # 额外的 container, 按 k8s 的语法填. 根据需求增加，如果没有需求，可以省略该字段
    - name: wahaha
      image: yixixi:1.0.b
      command: ["/bin/sh", "-c", "sleep 60"]
      ports:
        - containerPort: 28888
```

**螺丝配卡**

**问题描述**

输入：用户一般只需要根据使用场景进行配卡，有些时候会指定某些operator在固定台数上运行

```json
   "ragnaros_operator_config": {
        "face_feature_extract": { "CPU": 0, "P4": 1, "T4": 0 },
        "body_feature_extract": { "CPU": 0, "P4": 0, "T4": 0 },
        "vehicle_feature_extract":        { "P4": 1 },
        "face_1vn":             { "CPU": 1, "P4": 0, "T4": 0 },
        "body_1vn":             { "CPU": 1, "P4": 0, "T4": 0 },
        "vehicle_1vn":          { "CPU": 0, "P4": 0, "T4": 0 },
        "non_motor_vehicle_1vn":{ "CPU": 0, "P4": 0, "T4": 0 },
        "face_image_flow":      {
            "resource_num":     { "CPU": 0, "P4": 1, "T4": 0 },
            "machine_num":                { "P4": 0, "T4": 0 }
        },
        "face_video":           { "CPU": 0, "P4": 1, "T4": 0 },
        "security_pass":                  { "P4": 0, "T4": 0 }
    }    
```

输出:

配卡脚本要能根据用户的配置转为符合螺丝要求的yaml文件，即上述的values.yaml文件

总结

- 用户输入每个operator需要的资源总数(resource_num)和机器数量(machine_num)
- 输出的value.yaml中是一些类flamewaker的组合,每个flamewaker包含单个或者服务名称(group,另名service), 服务数量(instanceNumber,另名partion),单个服务的资源数量(gpu), 目前replica默认为1
- operator的名字与service是1:1或者1:n的关系,例如face_feature_extract operator包含(sample-simple-async 和 sample-simple-async-anno)两个service
- 如果用户指定了 machine_num,则partion == machine_num....machine_num一定能被resource_num整除, 与机器实体数无关(可大于)
- resource_num == partion*gpu

**问题难点**

- 不同operator的拆分理想条件可能不同，例如1vn希望使用尽可能少的partion数量,有利于提高性能，feature_extract希望尽可能多的partion数量, 这样可以平分在不同电脑上,充分利用cpu资源
- 当用户只指定resource_num总数时，需要将resource_num总数拆分成 单机gpu数量*partion数量的关系
- 多个operator拆分完成的单台gpu数量 * partion数量后,要验证该组合能够被成功调度到目前的集群上，尤其是集群存在大量异构机器时
- 部分operator实际是由多个服务组合而成，例如face_feature_extract 包含(sample-simple-async 和 sample-simple-async-anno)，当该operator如果有多张卡时,实际的service要如何分配
- 部分operator可能会共享service，例如face_image_flow 和 face_video 共享 face_video_retrieval，这种特殊的service需要如何处理，（图片流和视频流都需要face_feature_extract（抽特征）和face-surveillance-retrieval（检索），一张p4卡最多能够跑2kw的人脸特征抽取）

**模型设计**

cpu/gpu资源模型设计(meta.resource)

不同的机器配有不同的显卡数，需要一种简单的表达方式可配置的方式来说明该机器是否为cpu、gpu机器,p4、t4,多少张卡

如下:"T4"表示该机器为"GPU-T4"类型资源,且含有8个该类资源

```
"P4":{
    "Type": "GPU-P4",
    "ResourceUnit": [
      0, 1, 2, 3,
      4, 5, 6, 7
    ]
  },         
```

operator模型设计(meta.operator)

- operator的services字段:一般包含一个或者多个service....其中某个service为关键服务...其他的相关服务

- operator的key_strategy字段:表示配卡过程中,该operator按照什么策略进行配卡.**machine**表示尽可能尽可能少的partion数量,**service**表示尽可能可能多的partion数量,**single**表示单个partion只包含单张卡(特殊的service)

- operator的link_strategy字段:表示当单个partion包含多张卡时,link服务如果与key服务共享

- - **share-0-0**表示与key服务共享所有的卡,只起一个link服务,且可使用所有的卡...
  - **share-2-3**表示每2张卡起一个link服务,最多起3个link服务....如果单个partion的卡数量小于2,仍会启动一个link服务
  - **detach-2-3**表示每2张卡link服务独占一张,主服务使用剩余的卡,最多起3个link服务....如果partion的卡数量小于2,则退化为share-2-3

- resource_num 和 machine_num为默认值

```json
    "face_feature_extract_P4":{
        "mode": "partitions",
        "key_strategy":"service",
        "link_strategy":"share-0-0",
        "services":{
            "key": "face-feature-extract",
            "link": [
                "face-feature-extract-anno",
                "face-feature-extract-multi",
                "general-object-detect-group",
                "general-feature-extract-group"
            ]
        },
        "resource_num":{ "CPU": 0, "P4": 0, "T4": 0 },
        "machine_num":{ "P4": 0, "T4": 0 }
    },
```

**配卡逻辑实现**

对于每种资源CPU、T4、P4，配卡的逻辑不同

**CPU**

先检查每个服务所需的CPU数不超过已有的CPU资源，检查通过则根据服务所需直接分配	CPU资源

**T4或P4**

为了充分发挥服务的性能，不同的服务有不同的配卡策略，目前有以下几种配卡策略：

- special：划分固定的机器数量，是在cluster_config.json中指定了machine_num的服务
- machine：尽可能集中在某些机器
- service：尽可能多的平均在多台机器
- single：每台机器一张卡

| 服务                    | 配卡需求                          | 用户输入          | 输出(CPU, GPU, MACHINE) |
| ----------------------- | --------------------------------- | ----------------- | ----------------------- |
| face_1vn                | (CPU, P4, T4)                     | (c, p, t)         |                         |
| body_1vn                | (CPU, P4, T4)                     | (c, p, t)         |                         |
| vehicle_1vn             | (CPU, P4, T4)                     | (c, p, t)         |                         |
| non_motor_vehicle_1vn   | (CPU, P4, T4)                     | (c, p, t)         |                         |
| security_pass           | (P4, T4)                          | (p, t)            |                         |
| face_feature_extract    | (CPU, P4, T4)                     | (c, p, t)         |                         |
| body_feature_extract    | (CPU, P4, T4)                     | (c, p, t)         |                         |
| vehicle_feature_extract | (P4)                              | (p)               |                         |
| face_image_flow         | (CPU, P4, T4, P4机器数, T4机器数) | (c, p, t, pn, tn) |                         |
| face_video              | (CPU, P4, T4)                     | (c, p, t)         |                         |

综合所有服务的需求：

对于服务a，分配策略为machine，需求p1

对于服务b，分配策略为service，需求p2，

对于服务c......

对于某一服务，尝试所有的划分方案

即p1 = 8 * a8，...， 2 * a2,  1 * a1，如对于8×a8，表示a8台机器上8卡

p2 = 1 *  b1, 2 * b2, ... , 8 * b8，

.....

为所有服务选取一种策略，如[8×a8, 1×b1，.......,]，这是一种分配策略

对于某种分配策略，计算分数：

score = service + machine

service = p *  k/n，k<=n，p是分配的总卡数，k是被切分的实例数，n是机器数

machine = p/k，p是分配的总卡数，k是被切分的实例数



score = serviceGood - serviceBad - machineScore

serviceGood = sum(cn)，所有service分配策略服务中分配机器数cn <= 配置的机器数

serviceBad = sum(cn-8)，所有service分配策略服务中分配机器数cn > 配置的机器数（这里应该是个bug，应该减去配置的机器数，而不是8）

machineScore =sum(cn)，cn是所有machine分配策略服务中分配的机器数

按照分数从大到小排序，分数越高，越符合分配策略，即machine尽可能集中在某些机器，service尽可能多的平均在多台机器：

[[score1, [策略1]]，[score2, [策略2]，...]

针对其中一种分配策略，采用贪心的策略，校验其是否能够合理分配，如果不能，尝试下一种分配方案

（1）我们有资源，按照从大到小进行排序：

resource = [r1, r2, r3, ...]

对于某一种策略，按照GPU从大到小排序：

strategy = [n1×a1, n2×b2, ...]

分配：

如果r1 < n1，该策略分配不合理，尝试下一种分配策略

如果r1 >= n1

resource = [r1-n1, r2, r3, ...]

strategy = [n1×(a1-1), n2×b2，...]

转到（1），继续分配

**问题**

在一些场景下，螺丝pod随机调度会不合理导致失败（e.g. 2台8卡机，四个pod配卡分别为5 4 4 3时，期望的调度结果是5+3和4+4，而如果4+3先调度到了一台机器上，5+4则会有一个pod出现gpu资源不够无法调度的情况）

针对此类情况，可以使用上述描述的贪心算法，针对需求GPU大的pod优先级越高。

k8s背景知识：Pod 优先级与抢占	Pods 可以有**优先级（Priority）**。 优先级体现的是当前 Pod 与其他 Pod 相比的重要程度。如果 Pod 无法被调度，则 调度器会尝试抢占（逐出）低优先级的 Pod，从而使得悬决的 Pod 可被调度。

解决方法：

- 使用kubectl apply -f ./priorityclass.yaml创建PriorityClass


```
  apiVersion: v1
  items:
  - apiVersion: scheduling.k8s.io/v1
    description: "Used for 8 gpus pod"
    kind: PriorityClass
    metadata:
      name: ragnaros-cluster-critical-8
    value: 800008000
  - apiVersion: scheduling.k8s.io/v1
    description: "Used for 7 gpus pod"
    kind: PriorityClass
    metadata:
      name: ragnaros-cluster-critical-7
    value: 800007000
  - apiVersion: scheduling.k8s.io/v1
    description: "Used for 6 gpus pod"
    kind: PriorityClass
    metadata:
      name: ragnaros-cluster-critical-6
    value: 800006000
```

- 在values.yaml.template加入priorityClassName

```
...{% endif %}
  podAttributes:    # 在这里添加 注意空格
    priorityClassName: ragnaros-cluster-critical-{{ gpus }}...
```

解决方法二

- 配置调度优先级，需求量大的优先
- 选择BinPack调度策略
  - BinPack优先分配pod到资源占用率高的node上
  - spread策略则是尽量均匀分配到不同node上
  - random随机调度

k8s Binpack资源调度

**性能测试**

问题

- 如何做压力控制，以产生恒定的QPS
- 如何充分利用多进程和协程以模拟实际生产环境的负载
- 如何得到较为准确的负载，该负载应该与服务所能承载的真实负载接近
- 面对很多不同的任务，如何设计一套通用的测试框架以尽可能提高代码的可复用性、维护性、扩展性

解决方法

实现框架

```python
## 线程安全的计数器
class Counter(object):
    def __init__(self):
        self.sent_cnt_lock = multiprocessing.Lock()
        self.resp_cnt_lock = multiprocessing.Lock()
        self.fail_cnt_lock = multiprocessing.Lock()
        self.active_user_cnt_lock = multiprocessing.Lock()
        self.get_image_sent_cnt_lock = multiprocessing.Lock()
        self.get_image_resp_cnt_lock = multiprocessing.Lock()
        self.get_image_fail_cnt_lock = multiprocessing.Lock()
        self.latency_list_lock = multiprocessing.Lock()
        self.sent_cnt = multiprocessing.Value('i', 0)
        self.resp_cnt = multiprocessing.Value('i', 0)
        self.fail_cnt = multiprocessing.Value('i', 0)
        self.active_user_cnt = multiprocessing.Value('i', 0)

        self.get_image_sent_cnt = multiprocessing.Value('i', 0)
        self.get_image_resp_cnt = multiprocessing.Value('i', 0)
        self.get_image_fail_cnt = multiprocessing.Value('i', 0)
        self.latency_list = multiprocessing.Manager().dict()

    def increment_sent_cnt(self):
        with self.sent_cnt_lock:
            self.sent_cnt.value += 1

    def value_sent_cnt(self):
        with self.sent_cnt_lock:
            return (self.sent_cnt.value)
    ## ...

class test():
    def __init__(self, task, start, step, target, process_num, coroutine_num):
        self.task = task # 测试任务
        self.step = stop
        ### 压力控制队列
        self.queue = multiprocessing.Queue()
        
    ## 检查活性
    def KeepAlive():
        pass
    
    ## 计算qps，失败检测
    def Check_Stats():
        pass
    
    ## 压力控制
    def LoadGenerater():
        sentcnt = 0
        queue = queue_in
        t0 = time.time()
        while True:
            if STOP_CURRENT_SCN.value:
                break
            if WANTED_QPS > 0:  # 表示恒定QPS
                num_req_to_be_sent = int((time.time() - t0) * WANTED_QPS - sentcnt)
                if num_req_to_be_sent > 0:
                    for i in range(num_req_to_be_sent):
                        queue.put(1)
                        sentcnt = sentcnt + 1
                time.sleep(1 / WANTED_QPS)
            else:
                time.sleep(3)
    
    def SCN_batch_retrieval():
        while True:
            ## 执行任务
            flag = task()
            ##统计成功失败
    
    
    ## 
    def run_coroutine_task():
        loop = asyncio.get_event_loop()
        tasks = []
        for i in range(COROUTINE_NUM):
            tasks.append(SCN_batch_retrieval())

        loop.run_until_complete(asyncio.gather(*tasks))
        loop.close()
        
    def start():
        process_list = []
        ## 每一秒做登录检查
        p = multiprocessing.Process(target=KeepAlive)
        p.start()
        process_list.append(p)

        ## 状态检查，QPS、latency计算，失败检查
        p = multiprocessing.Process(target=Check_Stats, args=(counter, queue,))
        p.start()
        process_list.append(p)

        ## 负载控制
        p = multiprocessing.Process(target=LoadGenerater, args=(queue,))
        p.start()
        process_list.append(p)
        
        ## 多进程多协程运行任务，充分利用计算资源
        for i in range(process_num):
            p = multiprocessing.Process(target=run_coroutine_task, args=(i, counter, queue))
            p.start()
            process_list.append(p)

        for p in process_list:
            p.join()
```

 性能测试

|                         | QPS       | 内存  | CPU  |
| ----------------------- | --------- | ----- | ---- |
| face_feature_extract    | 7500/8000 | 80/50 | 12/6 |
| body_feature_extract    |           |       |      |
| vehicle_feature_extract |           |       |      |
| face_1vn                | 1000/1100 | 70/60 | 12/7 |
| body_1vn                |           |       |      |
| vehicle_1vn             |           |       |      |
| non_motor_vehicle_1vn   |           |       |      |
| face_image_flow         |           |       |      |
| face_video              |           |       |      |
| security_pass           |           |       |      |